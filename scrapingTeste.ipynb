{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### a metade dos links da empresa escolhida para organização em uma futura adaptação, no qual ficaria dinâmico a empresa que fosse realizar o scraping\n",
    "\n",
    "link1 = 'faculdade-projecao/lista-reclamacoes/'\n",
    "link2 = 'faculdade-projecao/lista-reclamacoes/?pagina=2'\n",
    "link3 = 'faculdade-projecao/lista-reclamacoes/?pagina=3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importações necessárias\n",
    "\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pyautogui as pg\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from botcity.maestro import *\n",
    "from botcity.core import DesktopBot\n",
    "from botcity.web import WebBot, Browser, By\n",
    "import requests as rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utilizando a biblioteca pyautogui para abrir uma janela com 4 seleções: Página 1, Página 2, Página 3 e Cancelar\n",
    "\n",
    "pagina_scraping = pg.confirm(\n",
    "    title='Inicio do Scraping', \n",
    "    text='Escolha uma opção a seguir:',\n",
    "    buttons=['Página 1','Página 2', 'Página 3','Cancelar']    \n",
    ")\n",
    "\n",
    "match pagina_scraping:\n",
    "    case 'Página 1':\n",
    "        url_scraping = f'https://www.reclameaqui.com.br/empresa/{link1}'\n",
    "    case 'Página 2':\n",
    "        url_scraping = f'https://www.reclameaqui.com.br/empresa/{link2}'\n",
    "    case 'Página 3':\n",
    "        url_scraping = f'https://www.reclameaqui.com.br/empresa/{link3}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o scraping\n",
      "Scraping finalizado\n"
     ]
    }
   ],
   "source": [
    "# Inicia a aplicação da biblioteca Botcity\n",
    "\n",
    "botweb = WebBot()\n",
    "botweb.headless = True # True deixa a janela do navegador escondida durante o Scraping e False habilita\n",
    "botweb.browser = Browser.CHROME # Escolhe qual navegador deveja utilizar para o scraping\n",
    "botweb.driver_path = \"chromedriver.exe\" # Utiliza o chromedriver local, no qual já está habilitado dentro da pasta. Caso venha ter problemas com o chromedriver, tem que verificar qual a versão do seu google chrome e atualizar o arquivo do chromedriver\n",
    "\n",
    "# Cria as variáveis do tipo lista\n",
    "titulo_reclamacoes = []\n",
    "descricao_reclamacoes = []\n",
    "link_reclamacoes = []\n",
    "resposta_reclamacoes = []\n",
    "hora_reclamacoes = []\n",
    "\n",
    "# Cria a função extrair_informacoes com os dados necessários para a extração\n",
    "def extrair_informacoes(xpath, atributo, lista_destino):\n",
    "    elementos = botweb.find_elements(xpath, by=By.XPATH)\n",
    "    for elemento in elementos:\n",
    "        lista_destino.append(elemento.get_attribute(atributo))\n",
    "\n",
    "botweb.browse(url_scraping) # Abre o site estabelecido no campo de código anterior\n",
    "time.sleep(2) # Aguarda 2 segundos\n",
    "print(\"Iniciando o scraping\")\n",
    "    \n",
    "# Faz o scraping de cada informação de acordo com a variável que foi criada\n",
    "try:\n",
    "    extrair_informacoes('//h4[@class=\"sc-1pe7b5t-1 bVKmkO\"]', 'textContent', titulo_reclamacoes)\n",
    "    extrair_informacoes('//p[@class=\"sc-1pe7b5t-2 eHoNfA\"]', 'textContent', descricao_reclamacoes)\n",
    "    extrair_informacoes('//div[@class=\"sc-1pe7b5t-0 eJgBOc\"]//a', 'href', link_reclamacoes)\n",
    "    extrair_informacoes('//div[@class=\"sc-1pe7b5t-3 gkhqVK\"]//span[1]', 'textContent', resposta_reclamacoes)\n",
    "    extrair_informacoes('//div[@class=\"sc-1pe7b5t-3 gkhqVK\"]//span[2]', 'textContent', hora_reclamacoes)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Houve um erro em {e}\")\n",
    "    botweb.close_page()\n",
    "\n",
    "finally:\n",
    "    print(\"Scraping finalizado\")\n",
    "    botweb.close_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'titulo_reclamacao': titulo_reclamacoes,\n",
    "    'decricao': descricao_reclamacoes,\n",
    "    'links': link_reclamacoes,\n",
    "    'resposta_empresa': resposta_reclamacoes,\n",
    "    'hora_reclamacao': hora_reclamacoes    \n",
    "})\n",
    "\n",
    "json_data = df.to_json(orient='records', indent=1)\n",
    "json_data = json_data.replace(\"\\\\/\",\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code:  200\n",
      "Resposta do Servidor:  {'message': 'Workflow was started'}\n"
     ]
    }
   ],
   "source": [
    "data_to_send = {\n",
    "    \"dados_gerais\": json_data,\n",
    "    \"whatsapp\":\"sim\"\n",
    "}\n",
    "\n",
    "load_dotenv()\n",
    "url = os.getenv('url_wh')\n",
    "\n",
    "response = rq.post(url, data=data_to_send)\n",
    "\n",
    "print(\"Status code: \", response.status_code)\n",
    "print(\"Resposta do Servidor: \", response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
